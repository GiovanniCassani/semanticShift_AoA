rstan_options(silent = TRUE, open_progress = FALSE, show_messages = FALSE)
options(mc.cores = parallel::detectCores())
library(bayesplot)
library(tictoc)
library(gridExtra)
library(hypr)
library(grid)
library(intoo)
library(barsurf)
library(bivariate)
library(kableExtra)
library()
select <- dplyr::select
extract <- rstan::extract
if ( !('devtools' %in% installed.packages()) )
install.packages("devtools")
devtools::install_github("bearloga/tinydensR")
library(tinydensR)
univariate_discrete_addin()
univariate_continuous_addin()
?dbinom
dbinom(0, 3, 0.5)
dbinom(c(0,1,2,3), 3, 0.5)
# p(H==0 | H==1)
p[0] + p[1]
# p(H==c(0,1,2,3)) out of three tosses of a fair coin
probabilities = dbinom(c(0,1,2,3), 3, 0.5)
# p(H==0 | H==1)
probabilities[0] + probabilities[1]
probabilities[1]
# p(H==0 | H==1)
probabilities[1] + probabilities[2]
# p(H==0 | H==1 | H==2)
probabilities[1] + probabilities[2] + probabilities[3]
?rbinom
rbinom(1, 1, 0.5)
rbinom(1, 1, 0.5)
rbinom(1, 1, 0.5)
## 2.
x = NULL
for (i in 1:10) {
x[i] = rbinom(1, 1, 0.5)
}
sum(x)
x
successes = sum(x)
pbinom(2, 10, 0.5)
dbinom(2, 10, 0.5)
dbinom(1, 10, 0.5)
dbinom(2, 10, 0.5) + dbinom(1, 10, 0.5) + dbinom(0, 10, 0.5)
punif(2, 0, 10)
1 - punif(11, 0, 10)
dunif(6, 0, 10)
pnorm(3, mu, sigma)
mu = 100
sigma = 20
lb = 0
ub = 10
punif(2, lb, ub)
1 - punif(11, lb, ub)
dunif(6, lb, ub)
pnorm(3, mu, sigma)
1 - pnorm(11, mu, sigma)
dnorm(6, mu, sigma)
pbinom(2, n, pi)
pi = 0.6
n = 20
pbinom(2, n, pi)
1 - pbinom(11, n, pi)
dbinom(6, n, pi)
?Lognormal
x = rlnorm(1000, mu, sigma)
hist(x)
univariate_continuous_addin()
## 4.
mu = log(6)
sigma = log(2)
x = rlnorm(1000, mu, sigma)
hist(x)
pbinom(log(5), n, pi)
1 - pbinom(log(6), mu, sigma)
plnorm(log(5), n, pi)
1 - plnorm(log(6), mu, sigma)
## 4.
mu = 6
sigma = 2
grid = seq(0,25,.1)
x = rlnorm(1000, mu, sigma)
plot(grid,dlnorm(grid,1,.6),type="l",xlab="x",ylab="f(x)")
lines(density(x),col="red")
## 4.
mu = 6
sigma = 2
grid = seq(0,25,.1)
x = rlnorm(1000, mu, sigma)
plot(grid,dlnorm(grid,mu, sigma),type="l",xlab="x",ylab="f(x)")
lines(density(x),col="red")
density(x)
grid = seq(0,1000,.1)
plot(grid,dlnorm(grid,mu, sigma),type="l",xlab="x",ylab="f(x)")
grid = seq(0,10000,.1)
plot(grid,dlnorm(grid,mu, sigma),type="l",xlab="x",ylab="f(x)")
## 4.
mu = log(6)
sigma = log(2)
grid = seq(0,10000,.1)
plot(grid,dlnorm(grid,mu, sigma),type="l",xlab="x",ylab="f(x)")
grid = seq(0,100,.1)
plot(grid,dlnorm(grid,mu, sigma),type="l",xlab="x",ylab="f(x)")
plnorm(8, mu, sigma) - plnorm(2, mu, sigma)
plnorm(5, n, pi)
plnorm(5, mu, sigma)
1 - plnorm(6, mu, sigma)
plnorm(8, mu, sigma) - plnorm(2, mu, sigma)
## 5.
mu = 100
sigma = 10
plot(grid, dgamma(grid, mu, sigma), type='l', xlab='x', ylab='f(x)')
univariate_continuous_addin()
punif(3, lb, ub)
punif(3, lb, ub) - dunif(3, lb, ub)
pi = 0.6
n = 20
1 - pbinom(11, n, pi)
1 - pbinom(11, n, pi) + pbinom(11, n, pi)
plnorm(6, mu, sigma)
1 - plnorm(6, mu, sigma)
1 - plnorm(6, mu, sigma) + plnorm(6, mu, sigma)
x = seq(0, 1, by = 0.0001)
?dbeta
plot(x, dbeta(x, 10, 0), type='l')
plot(x, dbeta(x, 10, 1), type='l')
plot(x, dbeta(x, 0, 100), type='l')
plot(x, dbeta(x, 1, 100), type='l')
library(swirl)
install_course("Regression Models")
install_course("Statistical Inference")
# Visualize the binomial distribution
x <- seq(0,50,by=1) #Generate a series of values 0-50
y <- dbinom(x,50,prob=1/2)
plot(x,y, type='h', col='blue')
# If I request 50 tacos from a restaurant that serves both hard shell
# and soft shell tacos, what is the probability that 25 of them are soft shell?
y <- dbinom(x=25,size=50,prob=1/2)
print(y)
x <- seq(0,20,by=1) #Generate a series of values 0-20
y <- dbinom(x,20,prob=1/2)
plot(x,y, type='h', col='blue')
y<-dbinom(x=6,size=20,prob=1/2)
print(y)
# Netflix Question
x <- seq(0,50,by=1) #Generate a series of valuesn 0-20
y <- dbinom(x,50,prob=1/5)
plot(x,y, type='h', col='blue')
y<- dbinom(x=17,size=50,prob=1/5)
print(y)
# Queen of hearts from a deck of cards
queen<-dbinom(x=1,size=1,prob=1/52)
print(queen)
# Drawing a black figure from the deck
blck<-dbinom(x=1,size=1,prob=1/2)
print(blck)
# Draw 30 cards, with replacement, draw jack of spades 5 times
jack<-dbinom(x=5,size=30,prob=1/52)
print(jack)
# Generate 1000 random numbers with mean of 5 and sd 2
x<-rnorm(n=1000,mean=5,sd=2)
hist(x)
# cumulative prob of getting 25 soft tacos when I order 50 (hard or soft only)
sfttac<-pbinom(q=25,size=50,prob=1/2)
print(sfttac)
install.packages(c("lsr", "tigerstats"))
#sampling with and without replacement
pop<-base::letters[1:10]
samp1<-sample(pop,size=4,replace=FALSE)
samp2<-sample(pop,size=4,replace=TRUE)
bias<-c(1,1,0,0,0,0,1,1,0,0) #add a bias to letters a,b,g,h
samp3<-sample(pop,size=4,prob = bias,replace=FALSE)
#Generate three random variables with different sample sizes from a normal distribution
IQ.1 <- round(rnorm(n=100, mean=100, sd=15))
IQ.2 <- round(rnorm(n=1000,mean=100,sd=15 ))
IQ.3 <- round(rnorm(n=10000,mean=100,sd=15))
#look at the means and sds
mean(IQ.1)
sd(IQ.1)
mean(IQ.2)
sd(IQ.2)
mean(IQ.3)
sd(IQ.3)
#make a plot of the three means and sds
plot(c(mean(IQ.1),mean(IQ.2),mean(IQ.3)),type='l')
plot(c(sd(IQ.1),sd(IQ.2),sd(IQ.3)),type='l')
#code to generate many replications
x<-NULL
for (i in 1:10000){
x[i]<-round(rnorm(n=10000,mean=100,sd=15))
}
hist(x, freq=FALSE,col='gray')
#code to generate sampling distrubution of the mean
x<-NULL
for (i in 1:100){
x[i]<-mean(round(rnorm(n=5,mean=100,sd=15)))
}
hist(x, freq=FALSE,col='gray')
x<-NULL
for (i in 1:1000){
x[i]<-mean(round(rnorm(n=5,mean=100,sd=15)))
}
hist(x, freq=FALSE,col='gray')
x<-NULL
for (i in 1:10000){
x[i]<-mean(round(rnorm(n=5,mean=100,sd=15)))
}
hist(x, freq=FALSE,col='gray')
x<-NULL
for (i in 1:100){
x[i]<-mean(round(rnorm(n=10000,mean=100,sd=15)))
}
hist(x, freq=FALSE,col='gray')
x<-NULL
for (i in 1:1000){
x[i]<-mean(round(rnorm(n=10000,mean=100,sd=15)))
}
hist(x, freq=FALSE,col='gray')
x<-NULL
for (i in 1:10000){
x[i]<-mean(round(rnorm(n=10000,mean=100,sd=15)))
}
hist(x, freq=FALSE,col='gray')
# Calculate the standard error of the sampling distribution mean
x<-NULL
for (i in 1:10000){
x[i]<-sd(round(rnorm(n=5,mean=100,sd=15)))/sqrt(5)
}
hist(x, freq=FALSE,col='gray')
x<-NULL
for (i in 1:10000){
x[i]<-sd(round(rnorm(n=10000,mean=100,sd=15)))/sqrt(10000)
}
hist(x, freq=FALSE,col='gray')
#confidence intervals
qnorm( p = c(.025, .975) )
#T dist
N <- 10000 # suppose our sample size is 10,000
qt( p = .975, df = N-1)
# more sampling -----------------------------------------------------------
#https://homerhanumat.github.io/elemStats/sampling-and-surveys.html#types-of-samples
require(tigerstats)
data(FakeSchool)
#Set a seed to be able to replicate random samples
set.seed(23325)
#Generate a simple random sample of size 7
simplerandomsample<-popsamp(7,FakeSchool)
simplerandomsample
#Check mean of the population
mean.pop<- mean(FakeSchool$GPA)
mean.pop
#Check mean of the simple random sample
mean.srs<- mean(simplerandomsample$GPA)
mean.srs
#How many honors students vs non-honors?
summary(FakeSchool)
psych::describeBy(FakeSchool,FakeSchool$Honors)
set.seed(1837)
honors=subset(FakeSchool,Honors=="Yes")
honors
#Get sample from honors
honors.samp=popsamp(3,honors)
honors.samp
#Get non-honors sample
set.seed(17365)
nonhonors=subset(FakeSchool,Honors=="No")
nonhonors.samp=popsamp(4,nonhonors)
nonhonors.samp
#Combine
stratifiedsample<-rbind(honors.samp,nonhonors.samp)
stratifiedsample.mean<-mean(stratifiedsample$GPA)
stratifiedsample.mean
#Create sampling distribution of the mean and CIs for 100 simple random samples
x.100<-NULL
for (i in 1:100){
srs<-popsamp(7,FakeSchool)
x.100[i]<-mean(srs$GPA)
}
hist(x.100, freq=FALSE,col='gray')
mean(x.100)
ciMean(x.100)
library(lsr)
ciMean(x.100)
#Create sampling distribution of the mean and CIs for 1000 simple random samples
x.1000<-NULL
for (i in 1:1000){
srs<-popsamp(7,FakeSchool)
x.1000[i]<-mean(srs$GPA)
}
hist(x.1000, freq=FALSE,col='gray')
mean(x.1000)
ciMean(x.1000)
#Create sampling distribution of the mean and CIs for 10000 simple random samples
x.10000<-NULL
for (i in 1:10000){
srs<-popsamp(7,FakeSchool)
x.10000[i]<-mean(srs$GPA)
}
hist(x.10000, freq=FALSE,col='gray')
mean(x.10000)
ciMean(x.10000)
#load packages
install.packages("lsr") #only run this if you haven't installed it before
require('lsr')
require('datasets')
data("discoveries")
data('trees')
#trees
mean(trees$Girth)
sd(trees$Girth)
ciMean(x=trees$Girth,conf=.95)
ciMean(x=trees$Girth,conf=.99)
mean(trees$Height)
sd(trees$height)
ciMean(x=trees$Height,conf=.95)
ciMean(x=trees$Height,conf=.99)
mean(trees$Volume)
sd(trees$Volume)
ciMean(x=trees$Volume,conf=.95)
ciMean(x=trees$Volume,conf=.99)
#discoveries
mean(discoveries[1:100])
sd(discoveries[1:100])
ciMean(x=discoveries[1:100],conf=.95)
disc_samp1<-sample(discoveries[1:100],size=5,replace=FALSE)
mean(disc_samp1)
sd(disc_samp1)
disc_samp2<-sample(discoveries[1:100],size=10,replace=FALSE)
mean(disc_samp2)
sd(disc_samp2)
disc_samp3<-sample(discoveries[1:100],size=30,replace=FALSE)
mean(disc_samp3)
sd(disc_samp3)
disc_samp4<-sample(discoveries[1:100],size=60,replace=FALSE)
mean(disc_samp4)
sd(disc_samp4)
library(brms)
?get_prior
5.5 * 60 / 10
5.5 * 60
library(dplyr)
library(gvlma)
library(rockchalk)
items = read.delim("/Volumes/University/TiU/Research/Resources/BLP/blp-items.txt", header = T, sep = '\t')
stimuli = read.delim("/Volumes/University/TiU/Research/Resources/BLP/blp-stimuli.txt", header = T, sep = '\t')
blp.data <- merge(items, stimuli, by="spelling")
blp.data = select(blp.data, 'spelling', 'lexicality', 'rt', 'OLD20', 'subtlex.frequency')
blp.data$subtlex.frequency = log10(blp.data$subtlex.frequency + 1)
# let's log transform frequency to make it more normal
hist(blp.data$rt)
hist(blp.data$subtlex.frequency)
View(blp.data)
cor(blp.data)
cor(blp.data[,c('rt', 'OLD20', 'subtlex.frequency')])
# read in files
items = read.delim("/Volumes/University/TiU/Research/Resources/BLP/blp-items.txt",
header = T, sep = '\t')
stimuli = read.delim("/Volumes/University/TiU/Research/Resources/BLP/blp-stimuli.txt",
header = T, sep = '\t')
# merge files and select variables, then get rid of missing values
blp.data <- merge(items, stimuli, by="spelling")
blp.data = select(blp.data, 'spelling', 'lexicality', 'rt', 'OLD20', 'subtlex.frequency')
blp.data = blp.data[complete.cases(blp.data),]
# let's get a sense of the data and check for multicollinearity and correlations
psych::describe(blp.data)
str(blp.data)
cor(blp.data[,c('rt', 'OLD20', 'subtlex.frequency')])
blp.data$subtlex.frequency = log10(blp.data$subtlex.frequency + 1)
blp.data$log.rt = log10(blp.data$rt)
# let's start fitting models
lm.base = lm(log.rt ~ lexicality + OLD20, data = blp.data)
summary(lm.base)
extractAIC(lm.base)
lm.int = lm(log.rt ~ lexicality*OLD20, data = blp.data)
summary(lm.int)
extractAIC(lm.int)
anova(lm.base, lm.int)
plotSlopes(model = lm.int, plotx = 'OLD20', modx = 'lexicality', plotPoints = TRUE,
plotLegend = TRUE, legendArgs = list (x = "topleft"))
plotSlopes(model = lm.int, plotx = 'OLD20', modx = 'lexicality', plotPoints = TRUE,
plotLegend = TRUE, legendArgs = list (x = "topright"))
plotSlopes(model = lm.int, plotx = 'OLD20', modx = 'lexicality', plotPoints = F,
plotLegend = F)
View(items)
View(stimuli)
View(items)
View(stimuli)
# let's get a sense of the data and check for multicollinearity and correlations
psych::describe(blp.data)
str(blp.data)
cor(blp.data[,c('rt', 'OLD20', 'subtlex.frequency')])
# let's log transform rts and frequency to make them more normal
hist(blp.data$rt)
hist(blp.data$subtlex.frequency)
blp.data$subtlex.frequency = log10(blp.data$subtlex.frequency + 1)
# read in files
items = read.delim("/Volumes/University/TiU/Research/Resources/BLP/blp-items.txt",
header = T, sep = '\t')
stimuli = read.delim("/Volumes/University/TiU/Research/Resources/BLP/blp-stimuli.txt",
header = T, sep = '\t')
# merge files and select variables, then get rid of missing values
blp.data <- merge(items, stimuli, by="spelling")
blp.data = select(blp.data, 'spelling', 'lexicality', 'rt', 'OLD20', 'subtlex.frequency')
blp.data = blp.data[complete.cases(blp.data),]
# let's get a sense of the data and check for multicollinearity and correlations
psych::describe(blp.data)
str(blp.data)
cor(blp.data[,c('rt', 'OLD20', 'subtlex.frequency')])
# let's log transform rts and frequency to make them more normal
hist(blp.data$rt)
blp.data$log.rt = log10(blp.data$rt)
# let's start fitting models
lm.base = lm(log.rt ~ lexicality + OLD20, data = blp.data)
summary(lm.base)
100 + (100-0.3*100)
100 + (-0.3*100)
70 + (0.3*100)
70 + (0.3*70)
0.11*90
0.111*90
0.112*90
0.1111111111
0.111111*90
0.1111115*90
0.1111112*90
0.11111115*90
0.1111115*90
0.11111115*90
14.95 + 13.50
16 * 1.5
20: 100 = x:48
20*48/100
30*48/100
35*48/100
15*48/100
10*48/100
5 + 7 + 17 + 14.5 + 9.5
15*48/100
30*48/100
25*48/100
20*48/100
5 + 7 + 14.5 + 12 + 9.5
40*48/100
25*48/100
20*48/100
10*48/100
15*48/100
require(wec)
require(gvlma)
data(BMI)
# Dummy coding
dum<-psych::dummy.code(BMI$education)
bmi_ed<-data.frame(BMI,dum)
summary(bmi_ed)
# Predict BMI from using 'medium' education as the referent group
mod.1<-lm(BMI~lowest + highest, data=bmi_ed)
summary(mod.1)
View(BMI)
View(bmi_ed)
#Unweighted effects coding
contrasts(BMI$education)<-contr.sum(3)#use this to check out the contrast matrix
contr.sum(3)
contr.sum(4)
# Predict BMI from education using unweighted effects coding
mod.2<-lm(BMI~education, data=BMI,contrasts=list(education=contr.sum))
mod.2<-lm(BMI~education, data=BMI)
summary(mod.2)
8*0.6 + 9.5*0.4
install.packages("cocor")
library(dplyr)
library(cocor)
library(reshape2)
library(plyr)
library(ggplot2)
library(Rmisc)
library(MASS)
library(wec)
box_cox_transf <- function(x, min, max) {
bc = boxcox(x ~ 1, lambda = seq(min, max, 0.1))
df = data.frame(bc$x, bc$y)
df2 = df[with(df, order(-df$bc.y)),]
lambda = df2[1, "bc.x"]
print(lambda)
if (lambda != 0) {
x.transf = (x ^ lambda - 1)/lambda
} else {
x.transf = log(x)
}
return(x.transf)
}
setwd("/media/gioca90/University/TiU/Research/Projects/semanticShift/")
setwd("/Volumes/University/TiU/Research/Projects/semanticShift/")
##### d=40, w=3, minCount=10 #####
df = read.csv("data/processed/LNC.fixed.minCount10/semanticShift_d40_w3.csv", header = T, sep = ',')
df = df %>%
dplyr::rename(
FpM = FpM.SUM,
VC = VC.SUM,
rVC = rVC.SUM,
J = J.SUM,
rJ = rJ.SUM,
LNC = LNC.SUM,
rLNC = rLNC.SUM
)
hist(df$Freq)
hist(box_cox_transf(df$Freq, -10, 10))
